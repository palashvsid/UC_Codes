{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining with Python: Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from urllib import request\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Questions\n",
    "========================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 1 (3-18)\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'Wheeles', 'When', 'Where', 'Wherefore', 'Wherein', 'Whereto', 'Whether', 'Which', \"Whil'st\", 'While', 'Whiles', 'Whilst', 'Who', 'Whose', 'Why', 'what', 'whatsoeuer', 'whelped', 'when', 'where', 'wherefore', 'wherein', 'whereof', 'whereto', 'wherfore', 'whet', 'whether', 'which', \"whil'st\", 'while', 'whilest', 'whilst', 'whisper', 'whit', 'whizzing', 'who', 'whole', 'wholsome', 'whom', 'whose', 'why']\n",
      "\n",
      " We can see that there are duplicated words in the list due to both punctuation and case distinctions, such as What/what and Whilst/Whil'st/whilst respectively. \n"
     ]
    }
   ],
   "source": [
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "tokens = word_tokenize(caesar)\n",
    "tokens\n",
    "print([w for w in sorted(set(tokens)) if re.search('^(wh|Wh)', w)])\n",
    "print(\"\\n\",\"We can see that there are duplicated words in the list due to both punctuation and case distinctions, such as What/what and Whilst/Whil'st/whilst respectively. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 2 (3-19)\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in file: \n",
      " ['fuzzy 53\\n', 'logic 75\\n', 'machine 43\\n', 'data 98\\n', 'learn 12\\n', 'python 15\\n', 'panda 16\\n', 'koala 1\\n', 'down 17\\n', 'the 101\\n', 'an 14\\n', 'a 67\\n', 'and 166\\n', 'list 15\\n', 'god 34\\n', 'sachin 14\\n', 'lord 12\\n', 'rings 45\\n', 'simple 18\\n', 'tough 17\\n', 'load 13']\n",
      "\n",
      "Final Output: \n",
      " [['fuzzy', 53], ['logic', 75], ['machine', 43], ['data', 98], ['learn', 12], ['python', 15], ['panda', 16], ['koala', 1], ['down', 17], ['the', 101], ['an', 14], ['a', 67], ['and', 166], ['list', 15], ['god', 34], ['sachin', 14], ['lord', 12], ['rings', 45], ['simple', 18], ['tough', 17]]\n"
     ]
    }
   ],
   "source": [
    "list1 = open(\"words.txt\", encoding=\"utf8\").readlines()\n",
    "print(\"Data in file: \\n\", list1)\n",
    "list2 = []\n",
    "for i in range(len(list1)-1):\n",
    "    list2.append([re.split('[ \\n]', list1[i])[0], int(re.split('[ \\n]', list1[i])[1])])\n",
    "print('\\nFinal Output: \\n', list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 3 (3-21)\n",
    "◑ Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['named', '.txt', '.zip', 'beagles', 'beagles', 'has', 'lawyers', 'headers', 'apologies', 'has', 'deleted', 'mh', '**', '!**', '**', 'lawyers', 'etext', \"what's\", 'things', 'disclaims', 'tells', 'copies', 'etext', 'using', '-tm', 'etext', 'paid', 'etext', 'receiving', 'etext', '-', '-tm', 'etext', 'tm', 'etexts', 'things', 'means', 'owns', 'royalties', 'rules', 'etext', \"'s\", 'trademark', 'etexts', 'expends', 'efforts', 'efforts', \"'s\", 'etexts', 'things', 'errors', 'damaged', 'etext', 'codes', 'described', 'etext', '-tm', 'etext', 'disclaims', 'costs', 'expenses', 'including', 'fees', 'etext', 'receiving', 'paid', 'electronically', 'electronically', '-', 'states', 'disclaimers', 'implied', 'warranties', 'disclaimers', 'exclusions', 'rights', 'directors', 'officers', 'members', 'agents', 'including', 'fees', 'etext', 'etext', '-tm', 'copies', 'etext', 'electronically', 'references', 'copies', 'things', 'requires', 'etext', 'etext', 'mark-up', 'including', 'cessing', 'hypertext', 'software', 'etext', 'characters', '_', 'characters', 'characters', 'hypertext', 'etext', 'displays', 'etext', 'processors', 'etext', 'etext', 'provisions', 'trademark', 'profits', 'using', 'taxes', \"don't\", 'profits', 'required', \"'\", 'accepts', 'contributions', 'machines', 'software', 'etexts', 'licenses', 'paid', '.', '@compuserve.com', '-', '-', '*', '*', '.', '.', '.', '*', \"'s\", 'computers', \"didn't\", 'compiled', 'containing', 'ratios', 'prohibiting', 'abridging', 'grievances', 'well-regulated', 'infringed', 'prescribed', 'persons', 'houses', 'papers', 'searches', 'seizures', 'violated', 'supported', 'describing', 'searched', 'persons', 'things', 'seized', 'held', 'cases', 'arising', 'forces', 'compelled', 'deprived', 'prosecutions', 'committed', 'ascertained', 'confronted', 'witnesses', 'obtaining', 'witnesses', 'suits', 'dollars', 'preserved', 're-examined', 'rules', 'required', 'fines', 'imposed', 'punishments', 'inflicted', 'rights', 'construed', 'others', 'retained', 'powers', 'delegated', 'prohibited']\n",
      "\n",
      " Looking at the words in this list, we can see that most of the words that are occuring on this page are generally inflected or derived words, proper nouns, or words with UK spellings.\n"
     ]
    }
   ],
   "source": [
    "def unknown(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    raw1 = re.findall(r'\\b[^A-Z\\s\\d]+\\b', raw)\n",
    "    all_words= nltk.corpus.words.words('en')\n",
    "    print([w for w in raw1 if w not in all_words])\n",
    "\n",
    "unknown(\"http://www.gutenberg.org/files/2/2.txt\")\n",
    "\n",
    "print(\"\\n\", \"Looking at the words in this list, we can see that most of the words that are occuring on this page are generally inflected or derived words, proper nouns, or words with UK spellings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 4 (3-23)\n",
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'do', \"n't\", 'do', 'that', 'sir', 'That', 'is', 'a', 'Class', 'II', 'violation', 'of', 'the', 'Code', 'of', 'Ethics', 'of', 'this', 'company']\n",
      "We can't work with «n't|\\w+» simply because \\w+ is greeedy and captures n too. To stop this, we need to either define word boundaries or add an explicit condition to capture do separately.\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"n't|do|\\w+\", \"Please don't do that, sir. That is a Class-II violation of the Code of Ethics of this company.\"))\n",
    "\n",
    "print (\"We can't work with «n't|\\w+» simply because \\w+ is greeedy and captures n too. To stop this, we need to either define word boundaries or add an explicit condition to capture do separately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 5 (3-24)\n",
    "◑ Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w3 d0n'1 1a|k anym0r35w33t! 1 8 my f00d5w33t! $10p 1h15 mu51c n0w5w33t!\n"
     ]
    }
   ],
   "source": [
    "text1 = []\n",
    "text = \"We don't talk anymore. I ate my food. Stop this music now.\"\n",
    "text = text.lower()\n",
    "text = text.replace('ate', '8')\n",
    "pattern = r'[eiolst]|\\.'\n",
    "for w in text:\n",
    "    if re.search(pattern, w):\n",
    "        if w == 'e':\n",
    "            w = '3'\n",
    "        elif w == 'i':\n",
    "            w = '1'\n",
    "        elif w == 'o':\n",
    "            w = '0'\n",
    "        elif w == 'l':\n",
    "            w = '|'\n",
    "        elif w == 's':\n",
    "            w = '5'\n",
    "        elif w == '.':\n",
    "            w = '5w33t!'\n",
    "        elif w == 't':\n",
    "            w = '1'\n",
    "    text1.append(w)\n",
    "text1 = ''.join(text1)\n",
    "text1 = re.sub(r'\\b5', '$', text1)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 6 (3-27)\n",
    "◑ Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hahhheeehaaehheeehaaeehehehheehaehaheeaehhaahaahhehahhahhhaaehhhahhhhaahheeaeheahahehaahhaeaeehheaaahhahhahhhheeaaehhhhhaahhhaeahehahhehehhheeehehhheehheehheehheaeahhhaeheeeehhheaheaahahhheeahhheehhaheheaaehhhhhhaaeaheeheeeaheehaeehhahaeeahhhahahehhhhhhehhhhaehahhhehhahaaehhhehehahaahhhehahhaehahaahheeeahhheaheaaheahhahahahhhhahhaeahaehahhaahahhehhhehaaheeahehahehaheeahhheaheaeahhehhhhhhaeaaeehhhhahah\n"
     ]
    }
   ],
   "source": [
    "text1= (choice(\"aehh \") for x in range(500))\n",
    "text1 = ''.join(text1)\n",
    "text1 = ''.join(text1.split(' '))\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 7 (3-31)\n",
    "◑ Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For loop:\n",
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n",
      "\n",
      " List Comprehension:\n",
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "list1 = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "print(\"\\n\",\"For loop:\")\n",
    "lengths=[]\n",
    "for i in range(len(list1)):\n",
    "    lengths.append(len(list1[i]))\n",
    "print(lengths)\n",
    "\n",
    "print(\"\\n\",\"List Comprehension:\")\n",
    "lengths = [len(w) for w in list1]\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 8 (3-32)\n",
    "◑ Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "<ol type=\"a\">\n",
    "  <li>Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.</li>\n",
    "  <li>Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.</li>\n",
    "  <li>Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace. </li>\n",
    "  <li>Print the words of silly in alphabetical order, one per line. </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.  ['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n",
      "b. eoldrnnnna\n",
      "c. newly formed bland ideas are inexpressible in an infuriating way\n",
      "d.\n",
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = silly.split(\" \")\n",
    "print('a. ', bland)\n",
    "print('b.', ''.join([w[1] for w in bland]))\n",
    "print('c.', ' '.join(bland))\n",
    "print ('d.')\n",
    "for w in sorted(bland): \n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
