{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining with Python: Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Questions\n",
    "========================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 1 (2-13)\n",
    "◑ What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of noun synsets that have no hyponyms are 79.67 %\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "total = len(list(wn.all_synsets('n')))\n",
    "no_hyponyms= 0\n",
    "\n",
    "for i in wn.all_synsets('n'):\n",
    "    if not i.hyponyms():\n",
    "        no_hyponyms+=1\n",
    "print(\"The percentage of noun synsets that have no hyponyms are\", round(no_hyponyms/total*100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 2 (2-14)\n",
    "◑ Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine[\\'a vehicle that takes people to and from hospitals\\', \\'a car that has a long body and rear door with space behind rear seat\\', \\'a car that is old and unreliable\\', \\'a car driven by a person whose job is to take passengers where they want to go in exchange for money\\', \\'a small and economical car\\', \\'a car that has top that can be folded or removed\\', \\'a car with two doors and front seats and a luggage compartment\\', \\'a car in which policemen cruise the streets; equipped with radiotelephonic communications to headquarters\\', \\'a car that is powered by electricity\\', \\'a car with relatively low fuel efficiency\\', \\'a car that resembles a convertible but has a fixed rigid top\\', \\'a car having a hatchback door\\', \\'an early term for an automobile\\', \\'a car modified to increase its speed and acceleration\\', \\'a car suitable for traveling over rough terrain\\', \\'large luxurious car; usually driven by a chauffeur\\', \\'a car that is lent as a replacement for one that is under repair\\', \\'a car that is even smaller than a subcompact car\\', \\'a small box-shaped passenger van; usually has removable seats; used as a family car\\', \\'the first widely available automobile powered by a gasoline engine; mass-produced by Henry Ford from 1908 to 1927\\', \\'a high-performance car that leads a parade of competing cars through the pace lap and then pulls off the course\\', \\'a fast car that competes in races\\', \\'an open automobile having a front seat and a rumble seat\\', \\'a car that is closed and that has front and rear seats and two or four doors\\', \\'a high-performance four-wheel drive car built on a truck chassis\\', \\'a small low car with a high-powered engine; usually seats two persons\\', \\'a steam-powered automobile\\', \"a car kept in dealers\\' stock for regular sales\", \\'a car smaller than a compact car\\', \\'large open car seating four with folding top\\', \\'a car that has been previously owned; not a new car\\'][\\'a self-propelled wheeled vehicle that does not run on rails\\']'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def supergloss(s):\n",
    "    a=s.definition()\n",
    "    b=s.hyponyms()\n",
    "    c=[]\n",
    "    for i in b:\n",
    "        c.append(i.definition())\n",
    "    d=s.hypernyms()\n",
    "    e=[]\n",
    "    for i in d:\n",
    "        e.append(i.definition())\n",
    "\n",
    "    return a+str(c)+str(e)\n",
    "\n",
    "supergloss(wn.synset('car.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 3 (2-16)\n",
    "◑ Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in 1.1. Include the full set of Brown Corpus genres (nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest number of tokens per type)? Is this what you would have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 0.11953794237258804\n",
      "belles_lettres 0.09854647132227204\n",
      "editorial 0.1478637750795403\n",
      "fiction 0.1267375306623058\n",
      "government 0.10498167348859763\n",
      "hobbies 0.1314469609569494\n",
      "humor 0.21917492509794884\n",
      "learned 0.085085327234342\n",
      "lore 0.12151515426250464\n",
      "mystery 0.11305077926848467\n",
      "news 0.13039759731089762\n",
      "religion 0.15053681565521967\n",
      "reviews 0.19823604559748428\n",
      "romance 0.11257890377309988\n",
      "science_fiction 0.20953697304768487\n",
      "We can see that the category 'humor' is the one with the lowest diversity. This is close to what I'd have expected, since this means that humot is a category that requires lesser diversity in words and is a lower vocabulary intensive genre that others such as editorials or lore.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "for i in nltk.corpus.brown.categories():\n",
    "    words = len(set(w.lower() for w in brown.words(categories=i)))\n",
    "    vocab= len(brown.words(categories=i))\n",
    "    diversity = words/vocab\n",
    "    print (i, diversity)\n",
    "    \n",
    "print(\"We can see that the category 'humor' is the one with the lowest diversity. This is close to what I'd have expected, since this means that humot is a category that requires lesser diversity in words and is a lower vocabulary intensive genre that others such as editorials or lore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 4 (2-17)\n",
    "◑ Write a function that finds the 50 most frequently occurring words of a text that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 906),\n",
       " ('one', 889),\n",
       " ('like', 624),\n",
       " ('upon', 538),\n",
       " ('man', 508),\n",
       " ('ship', 507),\n",
       " ('Ahab', 501),\n",
       " ('ye', 460),\n",
       " ('old', 436),\n",
       " ('sea', 433),\n",
       " ('would', 421),\n",
       " ('head', 335),\n",
       " ('though', 335),\n",
       " ('boat', 330),\n",
       " ('time', 324),\n",
       " ('long', 318),\n",
       " ('said', 302),\n",
       " ('yet', 300),\n",
       " ('still', 299),\n",
       " ('great', 293),\n",
       " ('two', 285),\n",
       " ('seemed', 283),\n",
       " ('must', 282),\n",
       " ('Whale', 282),\n",
       " ('last', 277),\n",
       " ('way', 269),\n",
       " ('Stubb', 255),\n",
       " ('see', 253),\n",
       " ('Queequeg', 252),\n",
       " ('little', 247),\n",
       " ('round', 242),\n",
       " ('whales', 237),\n",
       " ('say', 237),\n",
       " ('three', 237),\n",
       " ('men', 236),\n",
       " ('thou', 232),\n",
       " ('may', 230),\n",
       " ('us', 228),\n",
       " ('every', 222),\n",
       " ('much', 218),\n",
       " ('could', 215),\n",
       " ('Captain', 215),\n",
       " ('first', 210),\n",
       " ('side', 208),\n",
       " ('hand', 205),\n",
       " ('ever', 203),\n",
       " ('Starbuck', 196),\n",
       " ('never', 195),\n",
       " ('good', 192),\n",
       " ('white', 191)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here, I have removed stopwords and made sure all words are text only'''\n",
    "def fifty_most_words(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    cleaned_text = [w for w in text if w.lower() not in stopwords and w.isalpha()]\n",
    "    return FreqDist(cleaned_text).most_common(50)\n",
    "\n",
    "fifty_most_words(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 5 (2-18)\n",
    "◑ Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Sperm', 'Whale'), 119),\n",
       " (('Moby', 'Dick'), 84),\n",
       " (('old', 'man'), 80),\n",
       " (('White', 'Whale'), 74),\n",
       " (('Captain', 'Ahab'), 63),\n",
       " (('sperm', 'whale'), 56),\n",
       " (('mast', 'head'), 45),\n",
       " (('Right', 'Whale'), 39),\n",
       " (('mast', 'heads'), 36),\n",
       " (('ye', 'see'), 36),\n",
       " (('whale', 'ship'), 33),\n",
       " (('cried', 'Ahab'), 33),\n",
       " (('Captain', 'Peleg'), 32),\n",
       " (('Aye', 'aye'), 31),\n",
       " (('white', 'whale'), 31),\n",
       " (('Mr', 'Starbuck'), 29),\n",
       " (('quarter', 'deck'), 28),\n",
       " (('one', 'hand'), 28),\n",
       " (('whale', 'boat'), 26),\n",
       " (('one', 'side'), 23),\n",
       " (('cried', 'Stubb'), 23),\n",
       " (('every', 'one'), 21),\n",
       " (('let', 'us'), 20),\n",
       " (('never', 'mind'), 20),\n",
       " (('chief', 'mate'), 19),\n",
       " (('ye', 'ye'), 19),\n",
       " (('years', 'ago'), 18),\n",
       " (('New', 'Bedford'), 18),\n",
       " (('said', 'Stubb'), 18),\n",
       " (('cried', 'Starbuck'), 17),\n",
       " (('something', 'like'), 16),\n",
       " (('Cape', 'Horn'), 16),\n",
       " (('would', 'seem'), 16),\n",
       " (('lower', 'jaw'), 16),\n",
       " (('Look', 'ye'), 16),\n",
       " (('thee', 'thou'), 16),\n",
       " (('old', 'Ahab'), 16),\n",
       " (('round', 'round'), 16),\n",
       " (('try', 'works'), 16),\n",
       " (('well', 'known'), 15),\n",
       " (('three', 'years'), 15),\n",
       " (('would', 'fain'), 15),\n",
       " (('ivory', 'leg'), 15),\n",
       " (('Dough', 'Boy'), 15),\n",
       " (('boat', 'crew'), 15),\n",
       " (('Fast', 'Fish'), 15),\n",
       " (('three', 'four'), 14),\n",
       " (('whaling', 'voyage'), 14),\n",
       " (('thou', 'art'), 14),\n",
       " (('thus', 'far'), 14)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here, I have removed stopwords and made sure all words are text only'''\n",
    "def fifty_most_words(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    cleaned_text = [w for w in text if w.lower() not in stopwords and w.isalpha()]\n",
    "    return FreqDist(nltk.bigrams(cleaned_text)).most_common(50)\n",
    "\n",
    "fifty_most_words(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 6 (2-20)\n",
    "◑ Write a function word_freq() that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The frequency of name in the section news is 15'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_freq(word, section):\n",
    "    from nltk.corpus import brown\n",
    "    frequency=0\n",
    "    for w in brown.words(categories=section):\n",
    "        if w==word:\n",
    "            frequency+=1\n",
    "    return \"The frequency of \"+word+\" in the section \"+section+\" is \"+str(frequency)\n",
    "word_freq(\"name\", \"news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Question 7 (2-22)\n",
    "◑ Define a function hedge(text) which processes a text and produces a new version with the word 'like' between every third word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby like Dick by Herman like Melville 1851>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hedge(text):\n",
    "    counter=0\n",
    "    a=[]\n",
    "    for w in text:\n",
    "        counter+=1\n",
    "        if counter%3==0:\n",
    "            a.append(\"like\")\n",
    "            a.append(w)\n",
    "        else:\n",
    "            a.append(w)\n",
    "    return nltk.Text(a)    \n",
    "\n",
    "hedge(text1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
